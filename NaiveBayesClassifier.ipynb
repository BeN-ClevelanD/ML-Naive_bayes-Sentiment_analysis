{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \"\"\"\n",
    "    Naive Bayes classifier implementation.\n",
    "    Parameters:\n",
    "    - smoothing_factor (float): Smoothing factor for Laplace smoothing. Default is 1.0.\n",
    "    Attributes:\n",
    "    - smoothing_factor (float): Smoothing factor for Laplace smoothing.\n",
    "    - class_priors (dict): Dictionary to store the prior probabilities of each class.\n",
    "    - feature_likelihoods (dict): Dictionary to store the likelihood probabilities of each feature for each class.\n",
    "    - classes (array-like): Array to store the unique classes.\n",
    "    - num_features (int): Number of features in the dataset.\n",
    "    Methods:\n",
    "    - fit(X, y): Fit the Naive Bayes classifier to the training data.\n",
    "    - predict(X): Predict the class labels for the given input data.\n",
    "    - predict_instance(x): Predict the class label for a single instance.\n",
    "    \"\"\"\n",
    "    def __init__(self, smoothing_factor=1.0):\n",
    "        self.smoothing_factor = smoothing_factor\n",
    "        self.class_priors = {}\n",
    "        self.feature_likelihoods = {}\n",
    "        self.classes = None\n",
    "        self.num_features = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.num_features = X.shape[1]\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        for c in self.classes:\n",
    "            self.class_priors[c] = np.log(np.sum(y == c) / n_samples)\n",
    "\n",
    "        for c in self.classes:\n",
    "            class_samples = X[y == c]\n",
    "            feature_count = np.sum(class_samples, axis=0) + self.smoothing_factor\n",
    "            total_count = np.sum(feature_count)\n",
    "            self.feature_likelihoods[c] = np.log(feature_count / total_count)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_instance(x) for x in X])\n",
    "\n",
    "    def predict_instance(self, x):\n",
    "        scores = {}\n",
    "        for c in self.classes:\n",
    "            scores[c] = self.class_priors[c] + np.sum(x * self.feature_likelihoods[c])\n",
    "        return max(scores, key=scores.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer():\n",
    "    class BPETokenizer:\n",
    "        \"\"\"\n",
    "        BPE Tokenizer class for training and tokenizing text using Byte Pair Encoding (BPE) algorithm.\n",
    "        Args:\n",
    "            sentences (list): List of sentences to train the tokenizer.\n",
    "            vocab_size (int): The desired size of the vocabulary.\n",
    "        Attributes:\n",
    "            sentences (list): List of sentences to train the tokenizer.\n",
    "            vocab_size (int): The desired size of the vocabulary.\n",
    "            word_freqs (defaultdict): Dictionary to store the frequency of each word.\n",
    "            splits (dict): Dictionary to store the splits of each word.\n",
    "            merges (dict): Dictionary to store the merges of word pairs.\n",
    "            vocab (set): Set to store the unique tokens in the vocabulary.\n",
    "        Methods:\n",
    "            train(): Trains the tokenizer by processing the sentences and learning merges.\n",
    "            compute_pair_freqs(): Computes the frequency of each word pair in the training data.\n",
    "            apply_merge(): Applies a merge operation to the splits of each word.\n",
    "            tokenize(text): Tokenizes the given text using the learned merges.\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, sentences, vocab_size):\n",
    "        self.sentences = sentences\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_freqs = defaultdict(int)\n",
    "        self.splits = {}\n",
    "        self.merges = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "    def train(self):\n",
    "        for sentence in tqdm(self.sentences, desc=\"Processing sentences\"):\n",
    "            for x in range(0, len(sentence)-1):\n",
    "                word = sentence[x]\n",
    "                if x >= 1:\n",
    "                    word = '_' + word\n",
    "                self.word_freqs[word] += 1\n",
    "                self.splits[word] = list(word)\n",
    "                self.vocab.update(list(word))\n",
    "\n",
    "        pbar = tqdm(total=self.vocab_size - len(self.vocab), desc=\"Learning merges\")\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            pair_freqs = self.compute_pair_freqs()\n",
    "            if not pair_freqs:\n",
    "                break\n",
    "\n",
    "            best_pair = max(pair_freqs, key=pair_freqs.get)\n",
    "            new_token = ''.join(best_pair)\n",
    "            self.merges[best_pair] = new_token\n",
    "            self.vocab.add(new_token)\n",
    "\n",
    "            self.apply_merge(best_pair, new_token)\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "        return self.merges\n",
    "\n",
    "    def compute_pair_freqs(self):\n",
    "        pair_freqs = defaultdict(int)\n",
    "        for word, freq in self.word_freqs.items():\n",
    "            split = self.splits[word]\n",
    "            if len(split) == 1:\n",
    "                continue\n",
    "            for i in range(len(split) - 1):\n",
    "                pair = (split[i], split[i + 1])\n",
    "                pair_freqs[pair] += freq\n",
    "        return pair_freqs\n",
    "\n",
    "    def apply_merge(self, pair, new_token):\n",
    "        for word in self.splits:\n",
    "            split = self.splits[word]\n",
    "            new_split = []\n",
    "            i = 0\n",
    "            while i < len(split):\n",
    "                if i < len(split) - 1 and (split[i], split[i + 1]) == pair:\n",
    "                    new_split.append(new_token)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_split.append(split[i])\n",
    "                    i += 1\n",
    "            self.splits[word] = new_split\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = text.replace(' ', '_ ')\n",
    "        tokens = list(text)\n",
    "        for pair, merge in self.merges.items():\n",
    "            i = 0\n",
    "            while i < len(tokens) - 1:\n",
    "                if tokens[i] == pair[0] and tokens[i + 1] == pair[1]:\n",
    "                    tokens[i:i+2] = [merge]\n",
    "                else:\n",
    "                    i += 1\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vectorize(sentences, type2index):\n",
    "    vocab_size = len(type2index)\n",
    "    one_hot_sentences = []\n",
    "    for sentence in tqdm(sentences, desc=\"Creating one-hot vectors\"):\n",
    "        one_hot_sentence = np.zeros(vocab_size)\n",
    "        for word in sentence:\n",
    "            if word in type2index:\n",
    "                one_hot_sentence[type2index[word]] = 1\n",
    "        one_hot_sentences.append(one_hot_sentence)\n",
    "    one_hot_sentences = np.array(one_hot_sentences)\n",
    "    return one_hot_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(X_train, y_train):\n",
    "    print(\"Training classifier...\")\n",
    "    clf = NaiveBayes()\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "\n",
    "def evaluate_model(clf, X_test, y_test):\n",
    "    print(\"Evaluating model...\")\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    return accuracy, report\n",
    "\n",
    "def clean(text):\n",
    "    #text = re.sub(r'[^\\w\\s]', '', text) # Remove special characters\n",
    "    text = re.sub(r'http\\S+', '[URL]', text) # Replace URLS with [URL]\n",
    "    text = re.sub(r'\\d+', '[NUM]', text) # Replace numbers with [NUM]\n",
    "    text = text.strip() # Remove trailing spaces\n",
    "    return text.lower()\n",
    "\n",
    "def whitespace_tokenize(corpus):\n",
    "    return [sentence.split() for sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class whitespace_tokenizer:\n",
    "    \"\"\"\n",
    "    A tokenizer that splits text into tokens based on whitespace.\n",
    "    Args:\n",
    "        corpus (list): A list of sentences to build the vocabulary from.\n",
    "    Attributes:\n",
    "        corpus (list): The input corpus of sentences.\n",
    "        vocab (set): The set of unique words in the corpus.\n",
    "    Methods:\n",
    "        tokenize(text): Tokenizes the input text into a list of tokens.\n",
    "    Example:\n",
    "        corpus = [\"Hello world\", \"This is a sentence\"]\n",
    "        tokenizer = whitespace_tokenizer(corpus)\n",
    "        tokens = tokenizer.tokenize(\"Hello world\")\n",
    "        print(tokens)  # Output: ['Hello', 'world']\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "        self.vocab = set()\n",
    "        for sentence in corpus:\n",
    "            for word in sentence.split():\n",
    "                self.vocab.add(word)\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(train_df, dev_df, test_df, tokenizer):\n",
    "    \"\"\"\n",
    "    Perform sentiment analysis using a given tokenizer on the provided datasets.\n",
    "    Args:\n",
    "        train_df (pandas.DataFrame): The training dataset containing 'text' and 'label' columns.\n",
    "        dev_df (pandas.DataFrame): The development dataset containing 'text' and 'label' columns.\n",
    "        test_df (pandas.DataFrame): The test dataset containing 'text' and 'label' columns.\n",
    "        tokenizer: The tokenizer object used to tokenize the texts.\n",
    "    Returns:\n",
    "        tuple: A tuple containing the trained classifier and the type-to-index dictionary.\n",
    "    Raises:\n",
    "        None\n",
    "    Example:\n",
    "        train_df = pd.read_csv('train.csv')\n",
    "        dev_df = pd.read_csv('dev.csv')\n",
    "        test_df = pd.read_csv('test.csv')\n",
    "        tokenizer = Tokenizer()\n",
    "        clf, type2index = sentiment_analysis(train_df, dev_df, test_df, tokenizer)\n",
    "    \"\"\"\n",
    "    # Create type2index dictionary\n",
    "    type2index = {token: idx for idx, token in enumerate(tokenizer.vocab)}\n",
    "    \n",
    "    # Tokenize texts\n",
    "    train_tokenized = train_df['text'].progress_apply(lambda x: tokenizer.tokenize(x))\n",
    "    dev_tokenized = dev_df['text'].progress_apply(lambda x: tokenizer.tokenize(x))\n",
    "    test_tokenized = test_df['text'].progress_apply(lambda x: tokenizer.tokenize(x))\n",
    "    \n",
    "    # Create feature vectors\n",
    "    X_train = one_hot_vectorize(train_tokenized, type2index)\n",
    "    X_dev = one_hot_vectorize(dev_tokenized, type2index)\n",
    "    X_test = one_hot_vectorize(test_tokenized, type2index)\n",
    "    \n",
    "    # Prepare labels\n",
    "    y_train = (train_df['label'] == 'positive').astype(int)\n",
    "    y_dev = (dev_df['label'] == 'positive').astype(int)\n",
    "    y_test = (test_df['label'] == 'positive').astype(int)\n",
    "    \n",
    "    # Train classifier\n",
    "    clf = train_classifier(X_train, y_train)\n",
    "    \n",
    "    # Evaluate model on dev set\n",
    "    dev_accuracy, dev_report = evaluate_model(clf, X_dev, y_dev)\n",
    "    print(\"Dev Set Results:\")\n",
    "    print(f\"Accuracy: {dev_accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(dev_report)\n",
    "    \n",
    "    return clf, type2index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning text: 100%|██████████| 9260/9260 [00:00<00:00, 223216.68it/s]\n",
      "Cleaning text: 100%|██████████| 1781/1781 [00:00<00:00, 232364.55it/s]\n",
      "Cleaning text: 100%|██████████| 3514/3514 [00:00<00:00, 306241.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set up paths\n",
    "PROJECT_DIR = os.getcwd() + '/afrisent-semeval-2023'\n",
    "language = 'hau'\n",
    "DATA_DIR = f'{PROJECT_DIR}/data/{language}'\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "dev_df = pd.read_csv(f'{DATA_DIR}/dev.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "\n",
    "train_df = train_df[train_df['label'] != 'neutral']\n",
    "dev_df = dev_df[dev_df['label'] != 'neutral']\n",
    "test_df = test_df[test_df['label'] != 'neutral']\n",
    "\n",
    "# Preprocess data\n",
    "for df in [train_df, dev_df, test_df]:\n",
    "    tqdm.pandas(desc=\"Cleaning text\")\n",
    "    df['text'] = df['text'].progress_apply(clean)\n",
    "    \n",
    "# Prepare corpus for BPE\n",
    "train_corpus = train_df['text'].tolist()\n",
    "tokenized_train_corpus = whitespace_tokenize(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning text: 100%|██████████| 9260/9260 [00:00<00:00, 428613.66it/s]\n",
      "Cleaning text: 100%|██████████| 1781/1781 [00:00<00:00, 592250.49it/s]\n",
      "Cleaning text: 100%|██████████| 3514/3514 [00:00<00:00, 407148.74it/s]\n",
      "Creating one-hot vectors: 100%|██████████| 9260/9260 [00:00<00:00, 13787.70it/s]\n",
      "Creating one-hot vectors: 100%|██████████| 1781/1781 [00:00<00:00, 13487.85it/s]\n",
      "Creating one-hot vectors: 100%|██████████| 3514/3514 [00:00<00:00, 12302.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier...\n",
      "Evaluating model...\n",
      "Dev Set Results:\n",
      "Accuracy: 0.8674901740595171\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.87       894\n",
      "           1       0.90      0.83      0.86       887\n",
      "\n",
      "    accuracy                           0.87      1781\n",
      "   macro avg       0.87      0.87      0.87      1781\n",
      "weighted avg       0.87      0.87      0.87      1781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# White space tokenizer\n",
    "whitespace_tokenizer = whitespace_tokenizer(train_corpus)\n",
    "\n",
    "# Run sentiment analysis\n",
    "clf, type2index = sentiment_analysis(train_df, dev_df, test_df, whitespace_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPE tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 9260/9260 [00:00<00:00, 57303.12it/s]\n",
      "Learning merges: 100%|██████████| 507/507 [00:30<00:00, 16.81it/s]\n",
      "Cleaning text: 100%|██████████| 9260/9260 [00:21<00:00, 425.07it/s]\n",
      "Cleaning text: 100%|██████████| 1781/1781 [00:04<00:00, 369.52it/s]\n",
      "Cleaning text: 100%|██████████| 3514/3514 [00:07<00:00, 450.21it/s]\n",
      "Creating one-hot vectors: 100%|██████████| 9260/9260 [00:00<00:00, 99741.79it/s]\n",
      "Creating one-hot vectors: 100%|██████████| 1781/1781 [00:00<00:00, 139954.20it/s]\n",
      "Creating one-hot vectors: 100%|██████████| 3514/3514 [00:00<00:00, 119777.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier...\n",
      "Evaluating model...\n",
      "Dev Set Results:\n",
      "Accuracy: 0.8512071869736103\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.91      0.86       894\n",
      "           1       0.90      0.79      0.84       887\n",
      "\n",
      "    accuracy                           0.85      1781\n",
      "   macro avg       0.86      0.85      0.85      1781\n",
      "weighted avg       0.86      0.85      0.85      1781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train BPE\n",
    "bpe = BPETokenizer(tokenized_train_corpus, vocab_size=1000)\n",
    "merges = bpe.train()\n",
    "\n",
    "# Run sentiment analysis\n",
    "clf, type2index = sentiment_analysis(train_df, dev_df, test_df, bpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPE tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 9260/9260 [00:00<00:00, 61701.91it/s]\n",
      "Learning merges: 100%|██████████| 507/507 [00:33<00:00, 15.33it/s]\n",
      "Cleaning text: 100%|██████████| 9260/9260 [00:21<00:00, 430.34it/s]\n",
      "Cleaning text: 100%|██████████| 1781/1781 [00:04<00:00, 391.47it/s]\n",
      "Cleaning text: 100%|██████████| 3514/3514 [00:07<00:00, 475.37it/s]\n",
      "Creating one-hot vectors: 100%|██████████| 9260/9260 [00:00<00:00, 112109.29it/s]\n",
      "Creating one-hot vectors: 100%|██████████| 1781/1781 [00:00<00:00, 115635.53it/s]\n",
      "Creating one-hot vectors: 100%|██████████| 3514/3514 [00:00<00:00, 132554.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier...\n",
      "Evaluating model...\n",
      "Dev Set Results:\n",
      "Accuracy: 0.8512071869736103\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.91      0.86       894\n",
      "           1       0.90      0.79      0.84       887\n",
      "\n",
      "    accuracy                           0.85      1781\n",
      "   macro avg       0.86      0.85      0.85      1781\n",
      "weighted avg       0.86      0.85      0.85      1781\n",
      "\n",
      "Training BPE tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 9260/9260 [00:00<00:00, 48858.40it/s]\n",
      "Learning merges: 100%|██████████| 1507/1507 [01:35<00:00, 15.71it/s]\n",
      "Cleaning text: 100%|██████████| 9260/9260 [00:59<00:00, 154.49it/s]\n",
      "Cleaning text: 100%|██████████| 1781/1781 [00:12<00:00, 142.62it/s]\n",
      "Cleaning text: 100%|██████████| 3514/3514 [00:20<00:00, 170.92it/s]\n",
      "Creating one-hot vectors: 100%|██████████| 9260/9260 [00:00<00:00, 84648.83it/s]\n",
      "Creating one-hot vectors: 100%|██████████| 1781/1781 [00:00<00:00, 117147.94it/s]\n",
      "Creating one-hot vectors: 100%|██████████| 3514/3514 [00:00<00:00, 129502.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier...\n",
      "Evaluating model...\n",
      "Dev Set Results:\n",
      "Accuracy: 0.8500842223469961\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86       894\n",
      "           1       0.89      0.80      0.84       887\n",
      "\n",
      "    accuracy                           0.85      1781\n",
      "   macro avg       0.85      0.85      0.85      1781\n",
      "weighted avg       0.85      0.85      0.85      1781\n",
      "\n",
      "Training BPE tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 9260/9260 [00:00<00:00, 65444.68it/s]\n",
      "Learning merges: 100%|██████████| 4507/4507 [04:57<00:00, 15.17it/s]\n",
      "Cleaning text: 100%|██████████| 9260/9260 [02:50<00:00, 54.28it/s]\n",
      "Cleaning text: 100%|██████████| 1781/1781 [00:35<00:00, 49.90it/s]\n",
      "Cleaning text: 100%|██████████| 3514/3514 [00:59<00:00, 59.55it/s]\n",
      "Creating one-hot vectors: 100%|██████████| 9260/9260 [00:00<00:00, 49433.37it/s]\n",
      "Creating one-hot vectors: 100%|██████████| 1781/1781 [00:00<00:00, 44741.86it/s]\n",
      "Creating one-hot vectors: 100%|██████████| 3514/3514 [00:00<00:00, 57428.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier...\n",
      "Evaluating model...\n",
      "Dev Set Results:\n",
      "Accuracy: 0.8618753509264458\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.91      0.87       894\n",
      "           1       0.90      0.81      0.85       887\n",
      "\n",
      "    accuracy                           0.86      1781\n",
      "   macro avg       0.87      0.86      0.86      1781\n",
      "weighted avg       0.87      0.86      0.86      1781\n",
      "\n",
      "Training BPE tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 9260/9260 [00:00<00:00, 36310.38it/s]\n",
      "Learning merges: 100%|██████████| 9507/9507 [10:04<00:00, 15.72it/s]\n",
      "Cleaning text: 100%|██████████| 9260/9260 [05:21<00:00, 28.76it/s]\n",
      "Cleaning text: 100%|██████████| 1781/1781 [01:06<00:00, 26.83it/s]\n",
      "Cleaning text: 100%|██████████| 3514/3514 [01:59<00:00, 29.47it/s]\n",
      "Creating one-hot vectors: 100%|██████████| 9260/9260 [00:00<00:00, 32696.55it/s]\n",
      "Creating one-hot vectors: 100%|██████████| 1781/1781 [00:00<00:00, 27581.06it/s]\n",
      "Creating one-hot vectors: 100%|██████████| 3514/3514 [00:00<00:00, 32785.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier...\n",
      "Evaluating model...\n",
      "Dev Set Results:\n",
      "Accuracy: 0.8663672094329029\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.87       894\n",
      "           1       0.90      0.82      0.86       887\n",
      "\n",
      "    accuracy                           0.87      1781\n",
      "   macro avg       0.87      0.87      0.87      1781\n",
      "weighted avg       0.87      0.87      0.87      1781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in [1000, 2000, 5000, 10000]:\n",
    "    # Train BPE\n",
    "    bpe = BPETokenizer(tokenized_train_corpus, vocab_size=x)\n",
    "    merges = bpe.train()\n",
    "\n",
    "    # Run sentiment analysis\n",
    "    clf, type2index = sentiment_analysis(train_df, dev_df, test_df, bpe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
